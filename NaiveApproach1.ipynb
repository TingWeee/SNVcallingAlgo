{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8fdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from numpy import nan\n",
    "import glob\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b52eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive1(folder_path):\n",
    "    features = ['CHROM','POS','is_snp']\n",
    "    lst_df = []\n",
    "    for filename in glob.glob(folder_path + '/*vcf.gz'):\n",
    "        df = allel.vcf_to_dataframe(filename, fields = features)\n",
    "        df = df[df.is_snp == True]\n",
    "        lst_df.append(df)\n",
    "        \n",
    "    suffix = ['vs','fb','m2','vd']\n",
    "    keep_same = {'CHROM', 'POS'}\n",
    "    i =0 \n",
    "    for dfs in lst_df:\n",
    "        dfs.columns = ['{}{}'.format(c, '' if c in keep_same else '_'+suffix[i]) for c in dfs.columns]\n",
    "        i += 1\n",
    "        \n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right,on =['CHROM', 'POS'],\n",
    "                                            how = 'outer', suffixes = ('', '')),lst_df)\n",
    "    \n",
    "    df = merged_df[merged_df[['is_snp_vd','is_snp_fb','is_snp_m2','is_snp_vs']].isnull().sum(axis=1) < 3]\n",
    "    df['end']= df['POS']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97188574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bed file input \n",
    "def getF1(pred_file, truth_file):\n",
    "    gt_pred = pd.read_csv(pred_file, sep = \"\\t\", names = ['Chromo', 'start', 'end'])\n",
    "    gt_pred['pred'] = 1\n",
    "    gt_truth = pd.read_csv(truth_file, sep = \"\\t\", names = ['Chromo', 'start', 'end'])\n",
    "    gt_truth['truth'] = 1\n",
    "    combined = gt_truth.merge(gt_pred, on=['Chromo', 'start'], how='outer')\n",
    "    combined['truth'].fillna(0, inplace=True)\n",
    "    combined['pred'].fillna(0, inplace=True)\n",
    "    f1_score = metrics.f1_score(y_true = combined['truth'], y_pred = combined['pred'])\n",
    "    score = metrics.precision_recall_fscore_support(y_true = combined['truth'], y_pred = combined['pred'])\n",
    "    print(f'F1-score:{f1_score}')\n",
    "    print(f'Overall:{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7989b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1(pred_df, truth_file):\n",
    "    pred_df['pred'] = 1\n",
    "    gt_truth = pd.read_csv(truth_file, sep = \"\\t\", names = ['Chromo', 'start', 'end'])\n",
    "    gt_truth['truth'] = 1\n",
    "    gt_truth['Chromo'] = gt_truth['Chromo'].astype(str)\n",
    "    combined = gt_truth.merge(pred_df, on=['Chromo', 'start'], how='outer')\n",
    "    combined['truth'].fillna(0, inplace=True)\n",
    "    combined['pred'].fillna(0, inplace=True)\n",
    "    f1_score = metrics.f1_score(y_true = combined['truth'], y_pred = combined['pred'])\n",
    "    precision_score = metrics.precision_score(y_true = combined['truth'], y_pred = combined['pred'])\n",
    "    recall_score = metrics.recall_score(y_true = combined['truth'], y_pred = combined['pred'])\n",
    "\n",
    "\n",
    "    print(f'F1-score: {f1_score}')\n",
    "    print(f'Precision: {precision_score}')\n",
    "    print(f'Recall: {recall_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc78b337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real1\n",
      "F1-score: 0.0006200159752311822\n",
      "Precision: 0.00031010967487475003\n",
      "Recall: 0.9454131918119788\n",
      "syn1\n",
      "F1-score: 0.001740502966811346\n",
      "Precision: 0.0008710127117900053\n",
      "Recall: 0.9957591178965225\n",
      "syn2\n",
      "F1-score: 0.0020651890382280746\n",
      "Precision: 0.0010336690735966135\n",
      "Recall: 0.9933056325023084\n",
      "syn3\n",
      "F1-score: 0.003766522400280735\n",
      "Precision: 0.0018868951857558946\n",
      "Recall: 0.9778565101860053\n",
      "syn4\n",
      "F1-score: 0.0074706839623875625\n",
      "Precision: 0.003751354831081303\n",
      "Recall: 0.8750842782715292\n",
      "syn5\n",
      "F1-score: 0.023130953237531105\n",
      "Precision: 0.011702940938450937\n",
      "Recall: 0.9846197915519027\n",
      "real2_part1\n",
      "F1-score: 0.0005291859015030521\n",
      "Precision: 0.00026468180391250314\n",
      "Recall: 0.7881873727087576\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "dataset_lst = ['real1', 'syn1', 'syn2', 'syn3', 'syn4', 'syn5', 'real2_part1']\n",
    "for dataset in dataset_lst:\n",
    "    df = naive1(dataset)\n",
    "    df = df[['CHROM','POS']]\n",
    "    df.rename(columns = {'CHROM':'Chromo', 'POS':'start'}, inplace = True)\n",
    "    print(dataset)\n",
    "    if dataset == 'real2_part1':\n",
    "        getF1(df, f'{dataset}/real2_truth_chr1to5.bed')\n",
    "    else:\n",
    "        getF1(df, f'{dataset}/{dataset}_truth.bed')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
